{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "seed= 42\n",
    "np.random.seed( seed )\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "\n",
    "# utility\n",
    "import os\n",
    "import sys\n",
    "from pprint import pprint\n",
    "from IPython.display import display\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import typing\n",
    "\n",
    "# visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLP\n",
    "import gensim\n",
    "\n",
    "# modelling and data\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "## NN\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, SpatialDropout1D, LSTM, Embedding\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# multiprocessing\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35513, 2)\n"
     ]
    }
   ],
   "source": [
    "path= os.path.join('/kaggle', 'input', 'obfuscated_text', 'xtrain_obfuscated.txt')\n",
    "xtrain= pd.read_csv(path, header= None)\n",
    "xtrain.columns= ['text']\n",
    "\n",
    "path= os.path.join('/kaggle', 'input', 'obfuscated_text', 'xtest_obfuscated.txt')\n",
    "xtest= pd.read_csv(path, header= None)\n",
    "xtest.columns= ['text']\n",
    "\n",
    "path= os.path.join('/kaggle', 'input', 'obfuscated_text', 'ytrain.txt')\n",
    "ytrain= pd.read_csv(path, header= None)\n",
    "ytrain.columns= ['label']\n",
    "\n",
    "data= pd.concat([xtrain, ytrain], axis= 1)\n",
    "data= pd.concat([data, xtest], axis= 0)\n",
    "print(data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>satwamuluhqgulamlrmvezuhqvkrpmletwulcitwskuhlemvtwamuluhiwiwenuhlrvimvqvkruhulenamuluhqgqvtwvimviwuhtwamuluhulqvkrenamcitwuhvipmpmqvuhskiwkrpmdfuhlrvimvskvikrpmqvuhskmvgzenleuhqvmvamuluhulenamuluhqvletwtwvipmpmgzleenamuhtwamuluhtwletwdfuhiwkrxeleentwxeuhpmqvuhtwiwmvamdfuhpkeztwamuluhvimvuhqvtwmkpmpmlelruhgztwtwskuhtwlrkrpmlruhpmuluhqvenuhtwyplepmxeuhenuhamypkrqvuhamulmvdfuhqvskentwamletwlrlrpmiwuhtwamul</td>\n",
       "      <td>7.0</td>\n",
       "      <td>406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>twmkiwpmqvtwleuhsaiwsktwmvlelekramuhqvkruhtwskenezuhskvienuhqgulvienulqvvimvuhvienuhvimvuhulyptwbrtvkrqvuhtwamuluhsktwlrvienamypuhqvmvamguuhvgoaulamlrmvvibhpmuluhqvijulmvnkuhqgskkrpmiwenuhsktwskskenuhsaiwmvleenulvikriwpmmkvimvuhiguhvgqgulleentwamuhsaezuhqvqvtwiwtvuhskvisknkuhravidfpmvitwleuhvienmvypqvpmjeuhxepmuhlekrtwulenezenuhiwenmvypvimvmkpmlegzuhsktwulenletvtwiwtwypuhtwamuluhpmul</td>\n",
       "      <td>3.0</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vidfpmskuhvilepmuluhtwtvuhulsovienamqvuhskiwmvamypuhtwamulsouhqgvienezuhtwamuluhamulmvdfuhsaiwulvitwiwpmmvmkuhlrvimviwlrlrkrleulqvuhqgiwlemvlruhtwamuluhsktwezentwleypqvuhsoqgulenamuluhlepmxeuhtwleenypuhulsovipmskuhiguhqgiwiwmvdfuhqgulenamuluhlepmxeuhtwleenypuhulsovipmskuhigsouhulqvvimvenlrenuhskentwamuhlekrpmsauhulmviwgzqvuhiwiwsoiguhlepmuhqgtwezuhezpmlexeuhxexepmuhskvienulxysouhuhragzqvenlelruhqvsoiwlemvlruhtwamul</td>\n",
       "      <td>8.0</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>raskleenkrlruhtwulenleengztwqvuhenuhsatvuhsktwskvikrpmlelekrqvuhsktwuciwendfuhamypentwuhskvienuhqgulleengzenuhulgztwucuhtwletwdfuhpmdfuluhtwamguuhralrvimvezpmypuhtwleenuhkrpmsauhdfpmviucuhsatwamulnkuhpkulypmvmkvipmypuhsaezuhpmuluhulvientwlrletwqvuhtwamuluhskmvenqvuhvgqgskleenpmtvuhvipmuhsktwulyptwgzcitwuhtwleenuhkrpmrbnkuhraleentwuhsaezuhtwskmvqvvimvuhlrvimvamultwezpmqvuhulqvlekrtvuhpmuluhsktweztwtwqvuhulenamuluhskvikrpmqv</td>\n",
       "      <td>3.0</td>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dfenqvuhtwamqvuhqgtwiwtvenuluhqvijletwamulenxeuhletwamuhleentwviuhvidfpmskuhulenqvuhtwamqvuhvitwamdfuhskvienuhpkvidfpmultwucpmeeuhqvskleendfpmuluhsktwucpmpmiwuhdfpmskvimvdfuhskuhuhralrvimvvilepmezuhsktwulvimvpmgzgzenuhtwamuluhvipmuhsktwlemvengztwleuhenqvmvkrpmxzuhpkskmviwbhvimvxexepmypuhenuhvipmgzkruhgzenleuhenuhtwucmviwuhulentwtvuhenuhamulmvdfuhskvipmyptwqvuhsaletwmktwuhsktwlekrqventwezuhamypmvamdfuhqgulmvuhvimv</td>\n",
       "      <td>4.0</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>qvsotwamuhulenamdfuhezpmlexeuhqgiwiwmvdfuhtwamuhtwqvlekrpmypuhxepmuhqvenuhuhjztwypvitwskmvmktwuhqvsolrvimvtwuhqvvilekruluhletwuliwpmdfuhskvienuhqgletwmkpmuhulqvtwkrszvimvuhtwamuluhultwlruhsatwamuluhxemvsouhuhjzqvlrentwuhskmvenqvuhsoqgvipmuhtwleenuhqvvipmmvqvqvtwqvuhtwamgusouhvienamuluhtwlepmezuhqvsotwletwamgusouhraqvgzmviwuhqvmvamuhlrvimvulmvtvuhqgsatvpmguuhsktwmkletwqvtvpmuhsoqgamqvenezqvuhenuhqvmvuhqvmvamguso</td>\n",
       "      <td>8.0</td>\n",
       "      <td>414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>iwmvenqvuhpmuluhsaendfuhtwamuluhqvmvuhqvmvamuluhoaamenlelekrlpuhraqvletwiwiwtwmkenleuluhsktwqvqvtwleulqvmvskuhtwmktwmviwtwlevtdfpmviucuhkrpmsauhqgqvlrpmskuhqvijskleenviletwdfuhraulxyvtviletwulqvenuhqvucleenamqvuhtwezpmqvuhdfenqvuhigvtulamlrmvleuhiwiwenvtsaiwulvitwqvtwlegzuhgzkruhsktwucypmvgzuhtwtvuhiwiwijtwsankuhpksatvuhulpmamqvuhtwamuhqvenuheztwamuluhvipmgzkruhtwypvieniwlr</td>\n",
       "      <td>7.0</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>twqvpmiwypuhpmuluhsktweztwtwqvuhqgskpmpmiwtvuhxepmuhlekrpmiwpmypuhtwamuluhtwucmviwuhqgskkrpmiwypuhsktwleuhenuhvitwamuluhskviskuhsooatwezuhgzmvamqvlepmdfuhskvienuhvidfpmskuhiwiwenxeuhiwiwmvdfuhkrpmsauhxemvuhqgqvtwlrenuhqvqvtwiwulvikrpmypuhamlrkrpmleamuluhqgletwulentwlelruhskvienuhtwlepmezuhsavienezuhskvienuhqgsaenuhqgkrpmsauhtwmkmvlruhiguhiwiwmvdfuhqvtwmkmviwuhtwqvtwamuluhiwiwsksouhjzlrvimvsaenqvuhtwtvuhpmuluhsktweztwtwqv</td>\n",
       "      <td>1.0</td>\n",
       "      <td>424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>iguhqgqvulqvmvlrpmiwpmvitwleamgzuhtwamuluhamulmvdfuhsaenezuhulmvuhqvenuhtwulenletwgzpmuhpmuluhulvimvamuhqvmvamuluhlrvimvmkentwiwuhulkrdfuhiwkrxeletwskvipmdfuhtwamuluhsatvuhsktwulenqvvitwgzezpmypuhvienamuluhtwlepmezuhqvmvuhletwgzpmlegzuhvimvenletvuhqvmvamuhxepmuhqvqvtwviiwiwenezqvuhtwmkmvulenleengzezpmypuhiwkrxeletwskvipmdfuhtwamuluhqgulamlrmviwuhqvmvamuluhvimvuhsktwdftwmvmkuhqglepmuyuhohsaiwiwenypmvlrpmiwpmvitwleamgz</td>\n",
       "      <td>7.0</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>vimvenlrenuhulkrpmuhtwezenypuhtwdfuhqgtwypmvleamuluhlepmuhtwypmvdfuluhulmvuhxepmuhskvikrpmleuhtwamuluhtwskenezuhskenamuhtwdfuhletwulxeenuhskvienuhqgtwqventwuhamulmvdfuhvimvuhlrvimvuciwendfuhlepmxeuhucvienleuhskvienuhvidfpmlelrletwmkpmuhpmpmuluhqvendfuhvitwskleenlruhtwamguuhlrvimvtwtvuhsatvuhulmvuhlrvimvulmvypmviwtwuhqvenuhxeiwtwqvsaezuhsktwskleenlrtwleuhulpmviuhskenamuhiguhxemvuhqgskmvskuhulmvuhvienamuluhtwlepmez</td>\n",
       "      <td>3.0</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                         text  \\\n",
       "0                      satwamuluhqgulamlrmvezuhqvkrpmletwulcitwskuhlemvtwamuluhiwiwenuhlrvimvqvkruhulenamuluhqgqvtwvimviwuhtwamuluhulqvkrenamcitwuhvipmpmqvuhskiwkrpmdfuhlrvimvskvikrpmqvuhskmvgzenleuhqvmvamuluhulenamuluhqvletwtwvipmpmgzleenamuhtwamuluhtwletwdfuhiwkrxeleentwxeuhpmqvuhtwiwmvamdfuhpkeztwamuluhvimvuhqvtwmkpmpmlelruhgztwtwskuhtwlrkrpmlruhpmuluhqvenuhtwyplepmxeuhenuhamypkrqvuhamulmvdfuhqvskentwamletwlrlrpmiwuhtwamul   \n",
       "1                                          twmkiwpmqvtwleuhsaiwsktwmvlelekramuhqvkruhtwskenezuhskvienuhqgulvienulqvvimvuhvienuhvimvuhulyptwbrtvkrqvuhtwamuluhsktwlrvienamypuhqvmvamguuhvgoaulamlrmvvibhpmuluhqvijulmvnkuhqgskkrpmiwenuhsktwskskenuhsaiwmvleenulvikriwpmmkvimvuhiguhvgqgulleentwamuhsaezuhqvqvtwiwtvuhskvisknkuhravidfpmvitwleuhvienmvypqvpmjeuhxepmuhlekrtwulenezenuhiwenmvypvimvmkpmlegzuhsktwulenletvtwiwtwypuhtwamuluhpmul   \n",
       "2          vidfpmskuhvilepmuluhtwtvuhulsovienamqvuhskiwmvamypuhtwamulsouhqgvienezuhtwamuluhamulmvdfuhsaiwulvitwiwpmmvmkuhlrvimviwlrlrkrleulqvuhqgiwlemvlruhtwamuluhsktwezentwleypqvuhsoqgulenamuluhlepmxeuhtwleenypuhulsovipmskuhiguhqgiwiwmvdfuhqgulenamuluhlepmxeuhtwleenypuhulsovipmskuhigsouhulqvvimvenlrenuhskentwamuhlekrpmsauhulmviwgzqvuhiwiwsoiguhlepmuhqgtwezuhezpmlexeuhxexepmuhskvienulxysouhuhragzqvenlelruhqvsoiwlemvlruhtwamul   \n",
       "3  raskleenkrlruhtwulenleengztwqvuhenuhsatvuhsktwskvikrpmlelekrqvuhsktwuciwendfuhamypentwuhskvienuhqgulleengzenuhulgztwucuhtwletwdfuhpmdfuluhtwamguuhralrvimvezpmypuhtwleenuhkrpmsauhdfpmviucuhsatwamulnkuhpkulypmvmkvipmypuhsaezuhpmuluhulvientwlrletwqvuhtwamuluhskmvenqvuhvgqgskleenpmtvuhvipmuhsktwulyptwgzcitwuhtwleenuhkrpmrbnkuhraleentwuhsaezuhtwskmvqvvimvuhlrvimvamultwezpmqvuhulqvlekrtvuhpmuluhsktweztwtwqvuhulenamuluhskvikrpmqv   \n",
       "4            dfenqvuhtwamqvuhqgtwiwtvenuluhqvijletwamulenxeuhletwamuhleentwviuhvidfpmskuhulenqvuhtwamqvuhvitwamdfuhskvienuhpkvidfpmultwucpmeeuhqvskleendfpmuluhsktwucpmpmiwuhdfpmskvimvdfuhskuhuhralrvimvvilepmezuhsktwulvimvpmgzgzenuhtwamuluhvipmuhsktwlemvengztwleuhenqvmvkrpmxzuhpkskmviwbhvimvxexepmypuhenuhvipmgzkruhgzenleuhenuhtwucmviwuhulentwtvuhenuhamulmvdfuhskvipmyptwqvuhsaletwmktwuhsktwlekrqventwezuhamypmvamdfuhqgulmvuhvimv   \n",
       "5              qvsotwamuhulenamdfuhezpmlexeuhqgiwiwmvdfuhtwamuhtwqvlekrpmypuhxepmuhqvenuhuhjztwypvitwskmvmktwuhqvsolrvimvtwuhqvvilekruluhletwuliwpmdfuhskvienuhqgletwmkpmuhulqvtwkrszvimvuhtwamuluhultwlruhsatwamuluhxemvsouhuhjzqvlrentwuhskmvenqvuhsoqgvipmuhtwleenuhqvvipmmvqvqvtwqvuhtwamgusouhvienamuluhtwlepmezuhqvsotwletwamgusouhraqvgzmviwuhqvmvamuhlrvimvulmvtvuhqgsatvpmguuhsktwmkletwqvtvpmuhsoqgamqvenezqvuhenuhqvmvuhqvmvamguso   \n",
       "6                                                    iwmvenqvuhpmuluhsaendfuhtwamuluhqvmvuhqvmvamuluhoaamenlelekrlpuhraqvletwiwiwtwmkenleuluhsktwqvqvtwleulqvmvskuhtwmktwmviwtwlevtdfpmviucuhkrpmsauhqgqvlrpmskuhqvijskleenviletwdfuhraulxyvtviletwulqvenuhqvucleenamqvuhtwezpmqvuhdfenqvuhigvtulamlrmvleuhiwiwenvtsaiwulvitwqvtwlegzuhgzkruhsktwucypmvgzuhtwtvuhiwiwijtwsankuhpksatvuhulpmamqvuhtwamuhqvenuheztwamuluhvipmgzkruhtwypvieniwlr   \n",
       "7    twqvpmiwypuhpmuluhsktweztwtwqvuhqgskpmpmiwtvuhxepmuhlekrpmiwpmypuhtwamuluhtwucmviwuhqgskkrpmiwypuhsktwleuhenuhvitwamuluhskviskuhsooatwezuhgzmvamqvlepmdfuhskvienuhvidfpmskuhiwiwenxeuhiwiwmvdfuhkrpmsauhxemvuhqgqvtwlrenuhqvqvtwiwulvikrpmypuhamlrkrpmleamuluhqgletwulentwlelruhskvienuhtwlepmezuhsavienezuhskvienuhqgsaenuhqgkrpmsauhtwmkmvlruhiguhiwiwmvdfuhqvtwmkmviwuhtwqvtwamuluhiwiwsksouhjzlrvimvsaenqvuhtwtvuhpmuluhsktweztwtwqv   \n",
       "8        iguhqgqvulqvmvlrpmiwpmvitwleamgzuhtwamuluhamulmvdfuhsaenezuhulmvuhqvenuhtwulenletwgzpmuhpmuluhulvimvamuhqvmvamuluhlrvimvmkentwiwuhulkrdfuhiwkrxeletwskvipmdfuhtwamuluhsatvuhsktwulenqvvitwgzezpmypuhvienamuluhtwlepmezuhqvmvuhletwgzpmlegzuhvimvenletvuhqvmvamuhxepmuhqvqvtwviiwiwenezqvuhtwmkmvulenleengzezpmypuhiwkrxeletwskvipmdfuhtwamuluhqgulamlrmviwuhqvmvamuluhvimvuhsktwdftwmvmkuhqglepmuyuhohsaiwiwenypmvlrpmiwpmvitwleamgz   \n",
       "9            vimvenlrenuhulkrpmuhtwezenypuhtwdfuhqgtwypmvleamuluhlepmuhtwypmvdfuluhulmvuhxepmuhskvikrpmleuhtwamuluhtwskenezuhskenamuhtwdfuhletwulxeenuhskvienuhqgtwqventwuhamulmvdfuhvimvuhlrvimvuciwendfuhlepmxeuhucvienleuhskvienuhvidfpmlelrletwmkpmuhpmpmuluhqvendfuhvitwskleenlruhtwamguuhlrvimvtwtvuhsatvuhulmvuhlrvimvulmvypmviwtwuhqvenuhxeiwtwqvsaezuhsktwskleenlrtwleuhulpmviuhskenamuhiguhxemvuhqgskmvskuhulmvuhvienamuluhtwlepmez   \n",
       "\n",
       "   label  length  \n",
       "0    7.0     406  \n",
       "1    3.0     386  \n",
       "2    8.0     418  \n",
       "3    3.0     426  \n",
       "4    4.0     416  \n",
       "5    8.0     414  \n",
       "6    7.0     376  \n",
       "7    1.0     424  \n",
       "8    7.0     420  \n",
       "9    3.0     416  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['length']= data['text'].apply(len)\n",
    "display(data.head(10))\n",
    "plt.hist(data['length'], bins= 30)\n",
    "plt.title('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0     15.676806\n",
       "6.0     12.997878\n",
       "3.0     12.373512\n",
       "8.0     11.177068\n",
       "1.0     10.638821\n",
       "10.0     9.387014\n",
       "4.0      7.187894\n",
       "5.0      7.021807\n",
       "2.0      4.524344\n",
       "11.0     4.330575\n",
       "9.0      3.014179\n",
       "0.0      1.670101\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distribution of output classes\n",
    "data['label'].value_counts(normalize= True)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text normalization\n",
    "\n",
    "Although data sounds to be clean, but I will do some tests to see if it is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text= data['text'].iloc[0]\n",
    "# check if all are True\n",
    "sum([el.isalpha() for el in text]) == len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True    35513\n",
       "Name: quality_check, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['quality_check']= data['text'].apply( lambda row:\n",
    "                                        sum([el.isalpha() for el in row]) == len(row) )\n",
    "data['quality_check'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That just told me the text is completely clean. So we'll try to featurize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## featurizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_extractor(corpus, ngrams= (1,1)):\n",
    "    \"\"\"\n",
    "    corpus should be an array of documents. For example list\n",
    "    \"\"\"\n",
    "    vectorizer= CountVectorizer(analyzer='char', min_df= 1, ngram_range= ngrams)\n",
    "    features= vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, features\n",
    "\n",
    "def tfidf_transformer(bow_matrix):\n",
    "    transformer = TfidfTransformer(norm='l2', smooth_idf=True, use_idf=True)\n",
    "    tfidf_matrix = transformer.fit_transform(bow_matrix)\n",
    "    return transformer, tfidf_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(train: typing.List[str], \n",
    "                 test: typing.List[str],\n",
    "                 feature_type: str\n",
    "                 ) -> typing.Tuple[np.array, np.array, typing.List[str]]:\n",
    "    \"\"\"\n",
    "    Gets \n",
    "        1. train data: list of documents\n",
    "        2. test data: list of documents\n",
    "    returns\n",
    "        1. train: numpy array\n",
    "        2. test: numpy array\n",
    "        3. feature names: list of strings\n",
    "        \n",
    "    \"\"\"\n",
    "    if feature_type not in {'bow', 'tfidf'}:\n",
    "        raise ValueError(\"feature_type can only be 'bow' or 'tfidf'.\")\n",
    "    # first put train and test together\n",
    "    data= train.copy()\n",
    "    data.extend(test)\n",
    "    # get bow features, here bag of letters\n",
    "    bow_vectorizer, bow_features= bow_extractor(corpus= data,  ngrams= (1,1) )\n",
    "    # get tfidf features\n",
    "    tfidf_trans, tfidf_features= tfidf_transformer( bow_features )\n",
    "    if feature_type=='bow':\n",
    "        return ( bow_features.todense()[:len(train)],\n",
    "                 bow_features.todense()[len(train):],\n",
    "                 bow_vectorizer.get_feature_names() )\n",
    "    else:\n",
    "        return ( tfidf_features.todense()[:len(train)],\n",
    "                 tfidf_features.todense()[len(train):],\n",
    "                 bow_vectorizer.get_feature_names() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32513, 27)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>f</th>\n",
       "      <th>g</th>\n",
       "      <th>h</th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>k</th>\n",
       "      <th>l</th>\n",
       "      <th>m</th>\n",
       "      <th>n</th>\n",
       "      <th>o</th>\n",
       "      <th>p</th>\n",
       "      <th>q</th>\n",
       "      <th>r</th>\n",
       "      <th>s</th>\n",
       "      <th>t</th>\n",
       "      <th>u</th>\n",
       "      <th>v</th>\n",
       "      <th>w</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>34</td>\n",
       "      <td>41</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "      <td>43</td>\n",
       "      <td>39</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    a  b  c  d   e  f  g   h   i  j   k   l   m   n  o   p   q   r   s   t  \\\n",
       "0  17  0  2  4  25  4  5  34  16  0  16  34  41  10  0  17  15  16   7  23   \n",
       "1  12  2  0  1  28  1  8  30  24  2  22  24  30  16  1  14  12  10  14  19   \n",
       "\n",
       "    u   v   w  x  y  z  label  \n",
       "0  50  30  30  3  2  5      7  \n",
       "1  43  39  24  1  5  4      3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_test, feature_names= prepare_data(train= xtrain['text'].tolist(),\n",
    "                                            test= xtest['text'].tolist(),\n",
    "                                            feature_type= 'bow')\n",
    "df_train= pd.DataFrame(X_train, columns= feature_names)\n",
    "df_train['label']= ytrain\n",
    "print(df_train.shape)\n",
    "display(df_train.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Baseline model\n",
    "\n",
    "We have a multiclass classification problem. We can try to fit these models:\n",
    "\n",
    "1. Multinomial Naiive Bayes on bow features\n",
    "2. XGBoost (tfidf)\n",
    "3. FNN (tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naiive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, feature_names= prepare_data(train= xtrain['text'].tolist(),\n",
    "                                            test= xtest['text'].tolist(),\n",
    "                                            feature_type= 'bow')\n",
    "y_train= ytrain['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf= MultinomialNB(alpha= 0)\n",
    "scores= cross_val_score(estimator=clf, \n",
    "                        X=X_train, \n",
    "                        y=y_train, \n",
    "                        cv= 7, \n",
    "                        scoring= 'accuracy', \n",
    "                        n_jobs= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3066154519818124"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FitXGBoost:\n",
    "    \"\"\"\n",
    "    Fit XGBoost for regression & (multiclass)classification\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X, y, init_params, tuning_params, metric, cv= 5, seed= 42, \n",
    "                             problem_type='classification'):\n",
    "        self.X= X\n",
    "        self.y= y\n",
    "        self.init_params= init_params\n",
    "        self.tuning_params= tuning_params\n",
    "        self.metric= metric\n",
    "        self.cv= cv\n",
    "        self.seed= seed\n",
    "        self.problem_type= problem_type\n",
    "        if self.problem_type not in ['regression', 'classification']:\n",
    "            raise ValueError('accepted values for \"problem_type\" are either \"regression\" or \"classification.\"')\n",
    "    \n",
    "    def get_initial_ntrees(self) -> int:\n",
    "        \"\"\"\n",
    "        With some initial parameters, we first find the optimum number of trees\n",
    "        - returns the optimum initial number of trees\n",
    "        \"\"\"\n",
    "        if self.problem_type=='classification':\n",
    "            # compute class weights\n",
    "            classes= np.unique(self.y)\n",
    "            class_weights= list( class_weight.compute_class_weight('balanced', \n",
    "                                                                    classes,\n",
    "                                                                    self.y) )\n",
    "            class_weights= {k: class_weights[i] for i, k in enumerate(classes)}\n",
    "            w_array = np.ones(self.y.shape[0], dtype = 'float')\n",
    "            for i, val in enumerate(self.y):\n",
    "                w_array[i] = class_weights[val]\n",
    "        # build dataset special for xgboost \n",
    "        self.dtrain = xgb.DMatrix(self.X, label= self.y, weight= w_array )\n",
    "        \n",
    "        # find initial number of trees for a set of initial parameters\n",
    "        print('Finding the initial number of trees with the initial parameters...')\n",
    "        cv_results = xgb.cv( self.init_params, \n",
    "                             self.dtrain,  \n",
    "                             num_boost_round= 1000, \n",
    "                             seed= self.seed, \n",
    "                             nfold= self.cv,\n",
    "                             stratified= (self.problem_type == 'classification'),\n",
    "                             metrics= {self.metric},   \n",
    "                             early_stopping_rounds= 50,\n",
    "                             verbose_eval= 0)\n",
    "        return len(cv_results)\n",
    "        \n",
    "    def XGB_gridsearch(self, \n",
    "                       estimator: xgb.XGBModel, \n",
    "                       params: typing.Dict[str, typing.Any]\n",
    "                      )-> GridSearchCV:\n",
    "        \"\"\"\n",
    "        After finding the initial number of trees to start with, we have to adjust other hyper-\n",
    "        parameters as explained before with gridsearch\n",
    "        \"\"\"\n",
    "        if self.metric=='rmse':\n",
    "            scoring= 'neg_mean_squared_error'\n",
    "        elif self.metric == 'merror':\n",
    "            scoring= 'accuracy'\n",
    "        else:\n",
    "            scoring= self.metric            \n",
    "        # There is not a match between gridsearchcv scoring naming and that of xgboost naming\n",
    "        grid_search = GridSearchCV(estimator= estimator, \n",
    "                                   param_grid = params,\n",
    "                                   scoring= scoring, \n",
    "                                   cv = self.cv, \n",
    "                                   n_jobs= -1, \n",
    "                                   verbose = 2)\n",
    "        grid_search.fit(self.X, self.y)\n",
    "        return grid_search\n",
    "    ##############################################\n",
    "    def run(self) -> typing.Tuple[ typing.Dict[str, typing.Any], pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        1. finds the initial number of trees\n",
    "        2. optimizes hyperparameters based on tuning_params\n",
    "        3. finds the optimum number of trees with a low rating rate and \n",
    "            found hyperparameters\n",
    "        \n",
    "        It returns hyperparameters and cv_results\n",
    "        \"\"\"\n",
    "        ntrees= self.get_initial_ntrees()\n",
    "        # This is a base estimator for grid search\n",
    "        if self.problem_type =='regression':\n",
    "            xgb_model= xgb.XGBRegressor(random_state= self.seed )\n",
    "        elif self.problem_type =='classification':\n",
    "            xgb_model= xgb.XGBClassifier(random_state= self.seed )\n",
    "        params= self.init_params\n",
    "        params['learning_rate']= params['eta']; del(params['eta'])\n",
    "        params['n_estimators']= ntrees\n",
    "        # convert values into lists\n",
    "        for k, v in params.items():\n",
    "            params[k]= [v]\n",
    "\n",
    "        tuning_order= [ ['max_depth', 'min_child_weight'], ['gamma'], \n",
    "                       [ 'colsample_bytree', 'subsample'], ['reg_lambda'], ['reg_alpha'] ]\n",
    "        for stage in tuning_order:\n",
    "            for el in stage:\n",
    "                params[el]= self.tuning_params[el]\n",
    "\n",
    "            print('grid-searching for \" %s \"' %str(stage))\n",
    "            gs= self.XGB_gridsearch(estimator= xgb_model, params= params )\n",
    "            updated_params= gs.best_params_\n",
    "            updated_score= gs.best_score_\n",
    "            for el in stage:\n",
    "                params[el]= [ updated_params[el] ]\n",
    "            print('updated parameters...')\n",
    "            if self.problem_type=='regression':\n",
    "                updated_score= math.sqrt(-updated_score)\n",
    "            print('The current score for metric \"%s\" is: %0.4f ' %\n",
    "                                                  ( self.metric, updated_score ) ) \n",
    "            print('*'*10)\n",
    "\n",
    "        print('Found all the hyperparameters for the initial number of trees: ntrees= %d' %ntrees)\n",
    "        hyperparameters= params\n",
    "        for k,v in hyperparameters.items():\n",
    "            hyperparameters[k]= v[0] # delisting\n",
    "\n",
    "        print('Now we will start the process of learning by the found hyperparameters and by \\\n",
    "considering a low learning rate\\n')\n",
    "\n",
    "        hyperparameters['eta']= 0.01\n",
    "        del(hyperparameters['learning_rate'], hyperparameters['n_estimators'])\n",
    "\n",
    "        cv_results = xgb.cv( hyperparameters, \n",
    "                         self.dtrain,  \n",
    "                         num_boost_round= 3000, \n",
    "                         seed= self.seed, \n",
    "                         nfold= self.cv, \n",
    "                         stratified= (self.problem_type == 'classification'),\n",
    "                         metrics= { self.metric },   \n",
    "                         early_stopping_rounds= 100)\n",
    "        print('The optimum number of trees is: %d' %len(cv_results))\n",
    "        print('train score for metric \"%s\" is: %0.04f' % (self.metric, cv_results.iloc[-1, 0]))\n",
    "        print('cv score for metric \"%s\" is: %0.04f' % (self.metric, cv_results.iloc[-1, 2]))\n",
    "\n",
    "        # prepare for output\n",
    "        hyperparameters['learning_rate']= 0.01; del(hyperparameters['eta'])\n",
    "        hyperparameters['n_estimators']= len(cv_results)\n",
    "        print('The best hyperparameters are: \\n')\n",
    "        print(hyperparameters)\n",
    "        return hyperparameters, cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[ 0  1  2  3  4  5  6  7  8  9 10 11], y=[ 7  3  8 ... 10  6  3] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the initial number of trees with the initial parameters...\n"
     ]
    }
   ],
   "source": [
    "init_params= {\n",
    "                'eta':0.2,\n",
    "                'max_depth': 8,\n",
    "                'min_child_weight': 3,\n",
    "                'colsample_bytree': 0.8,\n",
    "                'subsample': 0.8,\n",
    "                'gamma': 0, \n",
    "                'reg_lambda': 1,\n",
    "                'reg_alpha': 0,\n",
    "                'objective':'multi:softmax',\n",
    "                'num_class': len(ytrain['label'].unique())}\n",
    "tuning_params= {\n",
    "                'learning_rate': [0.2],\n",
    "                'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "                'min_child_weight': [1, 2, 3, 4, 5],\n",
    "                'colsample_bytree': [i/10.0 for i in range(4,10)],\n",
    "                'subsample': [i/10.0 for i in range(6,10)],\n",
    "                'gamma': [i/10.0 for i in range(0,5)], \n",
    "                'reg_lambda': [1e-5, 1e-2, 0.1, 1, 10, 100],\n",
    "                'reg_alpha': [1e-5, 1e-2, 0.1, 1, 10, 100],\n",
    "                'objective': ['multi:softmax'],\n",
    "                'num_class': [len(ytrain['label'].unique())]}\n",
    "\n",
    "fit_xgboost= FitXGBoost(X= X_train,\n",
    "                        y= y_train,\n",
    "                        init_params= init_params.copy(),\n",
    "                        tuning_params= tuning_params.copy(),\n",
    "                        metric= 'merror',\n",
    "                        cv= 5,\n",
    "                        seed= seed,\n",
    "                        problem_type= 'classification')\n",
    "\n",
    "hyperparameters, cv_results= fit_xgboost.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tfidf data\n",
    "X_train, X_test, feature_names= prepare_data(train= xtrain['text'].tolist(),\n",
    "                                            test= xtest['text'].tolist(),\n",
    "                                            feature_type= 'tfidf')\n",
    "y_train= ytrain['label'].values\n",
    "\n",
    "xgb_clf= xgb.XGBClassifier(**hyperparameters, \n",
    "                       random_state= seed, \n",
    "                       verbosity= 1 )\n",
    "\n",
    "# get weights\n",
    "classes= np.unique(y_train)\n",
    "class_weights= list( class_weight.compute_class_weight('balanced', \n",
    "                                                        classes,\n",
    "                                                        y_train) )\n",
    "class_weights= {k: class_weights[i] for i, k in enumerate(classes)}\n",
    "w_array = np.ones(y_train.shape[0], dtype = 'float')\n",
    "for i, val in enumerate(y_train):\n",
    "    w_array[i] = class_weights[val]\n",
    "\n",
    "# fit the model    \n",
    "xgb_clf.fit(X_train, y_train, sample_weight= w_array)\n",
    "y_train_predicted= xgb_clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, feature_names= prepare_data(train= xtrain['text'].tolist(),\n",
    "                                            test= xtest['text'].tolist(),\n",
    "                                            feature_type= 'tfidf')\n",
    "y_train= ytrain['label']\n",
    "# needed for multiclass classification \n",
    "y_train= utils.to_categorical(y_train)\n",
    "\n",
    "print('X_train shape: ', X_train.shape)\n",
    "print('y_train shape: ', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_model():\n",
    "    kernel= 'normal'\n",
    "    model= Sequential(name= 'sequential')\n",
    "    #### layer 1\n",
    "    model.add(Dense(100, \n",
    "                    input_dim= 26, \n",
    "                    kernel_initializer= kernel, \n",
    "                    activation= 'relu', \n",
    "                    name= 'dense_1'))\n",
    "    model.add(Dropout(0.4, name= 'dropout_1'))\n",
    "    #### layer 2\n",
    "    model.add(Dense(50, \n",
    "                    kernel_initializer= kernel, \n",
    "                    activation= 'relu', \n",
    "                    name= 'dense_2'))\n",
    "    model.add(Dropout(0.4, name= 'dropout_2'))\n",
    "    #### layer 3\n",
    "    model.add(Dense(25, \n",
    "                    kernel_initializer= kernel, \n",
    "                    activation= 'relu', \n",
    "                    name= 'dense_3'))\n",
    "    model.add(Dropout(0.4, name= 'dropout_3'))\n",
    "    #### layer 4\n",
    "    model.add(Dense(12, \n",
    "                    kernel_initializer= kernel, \n",
    "                    activation= 'softmax', \n",
    "                    name= 'dense_4'))\n",
    "    model.compile(loss= 'categorical_crossentropy', optimizer= 'adam', metrics= ['accuracy'])\n",
    "    plot_model(model, show_shapes= True, to_file= 'FNN.png')\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_model(history: typing.Dict) -> None:\n",
    "    plt.figure(figsize= (12, 4))\n",
    "    plt.ylim(0, 1)\n",
    "    plt.plot(history['accuracy'], )\n",
    "    #plt.plot(history['val_accuracy'], marker= 'o', markersize= 4)\n",
    "    plt.title('model accuacy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='lower right')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure(figsize= (12, 4))\n",
    "    plt.plot(history['loss'], )\n",
    "    #plt.plot(history['val_loss'], marker= 'o', markersize= 4)\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper right')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= KerasClassifier(build_fn= NN_model,\n",
    "                        epochs= 100, \n",
    "                        batch_size= 128)\n",
    "scores= cross_val_score(estimator= model, \n",
    "                        X= X_train, \n",
    "                        y= y_train, \n",
    "                        cv= 5, \n",
    "                        n_jobs= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
